{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Install Reqirements"],"metadata":{"id":"__TIMlFTG50b"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtY4iVNoehNM","outputId":"eec431ce-88ef-41c3-cc87-30386d64a14e","executionInfo":{"status":"ok","timestamp":1677684316171,"user_tz":-360,"elapsed":26826,"user":{"displayName":"Cognition AI","userId":"09668512897373096866"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15287, done.\u001b[K\n","remote: Total 15287 (delta 0), reused 0 (delta 0), pack-reused 15287\u001b[K\n","Receiving objects: 100% (15287/15287), 14.12 MiB | 11.33 MiB/s, done.\n","Resolving deltas: 100% (10490/10490), done.\n","/content/yolov5\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\n","cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Setup complete. Using torch 1.13.1+cu116 (CPU)\n"]}],"source":["#clone YOLOv5 and \n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt # install dependencies\n","%pip install -q roboflow\n","\n","import torch\n","import os\n","from IPython.display import Image, clear_output  # to display images\n","\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"]},{"cell_type":"markdown","source":["## Assemble Our Dataset"],"metadata":{"id":"0ld9NLH9HM-t"}},{"cell_type":"code","source":["from roboflow import Roboflow\n","rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"],"metadata":{"id":"hjcgVzGYlBqk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f2f9baa2-e9e2-43aa-e03d-a10f16a2d00f","executionInfo":{"status":"ok","timestamp":1677684316720,"user_tz":-360,"elapsed":554,"user":{"displayName":"Cognition AI","userId":"09668512897373096866"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=ultralytics\n"]}]},{"cell_type":"code","source":["# set up environment\n","os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""],"metadata":{"id":"ZfhTKqlqHTeu","executionInfo":{"status":"ok","timestamp":1677684316720,"user_tz":-360,"elapsed":3,"user":{"displayName":"Cognition AI","userId":"09668512897373096866"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# *Import Dataset from roboflow*"],"metadata":{"id":"WJLIOdBbHkh-"}},{"cell_type":"code","source":["!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"Men7iaU4S0yEBbAEtI0v\")\n","project = rf.workspace(\"manarat-international-university\").project(\"bangla-sign-language-detection-system-using-yolov5\")\n","dataset = project.version(3).download(\"yolov5\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XcnzyZjpHWSm","outputId":"a98aeb2b-a1d2-48b5-d3dc-4d7ef8749f58","executionInfo":{"status":"ok","timestamp":1677684348133,"user_tz":-360,"elapsed":31416,"user":{"displayName":"Cognition AI","userId":"09668512897373096866"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: roboflow in /usr/local/lib/python3.8/dist-packages (0.2.32)\n","Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (0.10.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.26.14)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.8.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.22.4)\n","Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2022.12.7)\n","Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.8/dist-packages (from roboflow) (0.10.1)\n","Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.0.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.4.4)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.64.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from roboflow) (3.5.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.25.1)\n","Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.6.0.66)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.15.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from roboflow) (6.0)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.4.7)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from roboflow) (8.4.0)\n","Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (from roboflow) (3.2)\n","Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.10)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.0.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->roboflow) (23.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->roboflow) (4.38.0)\n","loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in /content/datasets/Bangla-Sign-Language-Detection-System-Using-YOLOv5-3 to yolov5pytorch: 100% [40741405 / 40741405] bytes\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Dataset Version Zip to /content/datasets/Bangla-Sign-Language-Detection-System-Using-YOLOv5-3 in yolov5pytorch:: 100%|██████████| 18382/18382 [00:04<00:00, 3974.25it/s]\n"]}]},{"cell_type":"markdown","source":["#**train the model**"],"metadata":{"id":"yWcqKeg0Hy-Q"}},{"cell_type":"code","source":["!python train.py --img 128 --batch 16 --epochs 200 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NK-nzzkdI9ay","outputId":"f41484f5-414b-4c69-b91e-a23c5aa7e208"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/datasets/Bangla-Sign-Language-Detection-System-Using-YOLOv5-3/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=200, batch_size=16, imgsz=128, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n","YOLOv5 🚀 v7.0-116-g5c91dae Python-3.8.10 torch-1.13.1+cu116 CPU\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","2023-03-01 15:25:53.334807: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-01 15:25:54.447303: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2023-03-01 15:25:54.447448: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n","2023-03-01 15:25:54.447472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 36.4MB/s]\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n","100% 14.1M/14.1M [00:00<00:00, 235MB/s]\n","\n","Overriding model.yaml nc=80 with nc=47\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1    140244  models.yolo.Detect                      [47, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model summary: 214 layers, 7146388 parameters, 7146388 gradients, 16.3 GFLOPs\n","\n","Transferred 343/349 items from yolov5s.pt\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/Bangla-Sign-Language-Detection-System-Using-YOLOv5-3/train/labels... 6427 images, 1 backgrounds, 0 corrupt: 100% 6427/6427 [00:09<00:00, 653.77it/s] \n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/Bangla-Sign-Language-Detection-System-Using-YOLOv5-3/train/labels.cache\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram): 100% 6427/6427 [00:03<00:00, 2014.60it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Bangla-Sign-Language-Detection-System-Using-YOLOv5-3/valid/labels... 1826 images, 0 backgrounds, 0 corrupt: 100% 1826/1826 [00:01<00:00, 1317.58it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/Bangla-Sign-Language-Detection-System-Using-YOLOv5-3/valid/labels.cache\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 1826/1826 [00:00<00:00, 2046.14it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.09 anchors/target, 0.999 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n","Plotting labels to runs/train/exp/labels.jpg... \n","Image sizes 128 train, 128 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp\u001b[0m\n","Starting training for 200 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      0/199         0G    0.09077    0.01365    0.09461         21        128: 100% 402/402 [07:09<00:00,  1.07s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 58/58 [01:28<00:00,  1.53s/it]\n","                   all       1826       1827    0.00353      0.855     0.0127    0.00372\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      1/199         0G    0.05718    0.01196    0.09132         19        128: 100% 402/402 [06:59<00:00,  1.04s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 58/58 [01:04<00:00,  1.12s/it]\n","                   all       1826       1827    0.00532      0.969     0.0358     0.0158\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      2/199         0G    0.05007    0.01056    0.09003         24        128: 100% 402/402 [06:51<00:00,  1.02s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 58/58 [01:06<00:00,  1.15s/it]\n","                   all       1826       1827     0.0401      0.332     0.0624     0.0276\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      3/199         0G    0.04574    0.01001    0.08818         21        128: 100% 402/402 [06:58<00:00,  1.04s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 58/58 [01:03<00:00,  1.09s/it]\n","                   all       1826       1827      0.349      0.164      0.083     0.0416\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      4/199         0G    0.04368   0.009824    0.08483         24        128: 100% 402/402 [06:55<00:00,  1.03s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 58/58 [01:01<00:00,  1.06s/it]\n","                   all       1826       1827      0.339      0.295      0.146     0.0808\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      5/199         0G    0.04262   0.009428    0.08203         38        128:  67% 269/402 [04:31<02:14,  1.01s/it]\n","Traceback (most recent call last):\n","  File \"train.py\", line 640, in <module>\n","    main(opt)\n","  File \"train.py\", line 529, in main\n","    train(opt.hyp, opt, device, callbacks)\n","  File \"train.py\", line 318, in train\n","    scaler.scale(loss).backward()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 488, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"code","source":["# Start tensorboard\n","# Launch after you have started training\n","# logs save in the folder \"runs\"\n","%load_ext tensorboard\n","%tensorboard --logdir runs"],"metadata":{"id":"a1FDGVUQ2v2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.1 --source {dataset.location}/test/images"],"metadata":{"id":"K1eKy1jTdvxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from google.colab import files\n","\n","# Path to the file you want to zip \n","file_path = '/content/yolov5/runs/train/exp5'\n","\n","# Name for the zip file\n","zip_name = 'exp5.zip'\n","\n","# Create the zip file\n","!zip {zip_name} {file_path}\n","\n","# Download the zip file\n","files.download(zip_name)\n","\n"],"metadata":{"id":"dsWOW7WQpH1H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XM7sIVBYdwXr"},"execution_count":null,"outputs":[]}]}